# ğŸ§  RAG Chatbot Document Search

A complete AI-powered document retrieval and question-answering system built with FastAPI, ChromaDB, and modern RAG techniques. This production-ready system supports multiple document formats, various LLM providers, and provides both REST API and Python SDK interfaces.

## âœ¨ Key Features

- **ğŸ” Multi-Format Document Support**: PDF, DOCX, PPTX, TXT, images, and archives with intelligent text extraction
- **ğŸ¤– Multiple LLM Providers**: OpenAI GPT, Anthropic Claude, Groq (ultra-fast inference), or mock providers for testing
- **ğŸ§® Advanced Embeddings**: OpenAI, Sentence-BERT, or mock embedding providers
- **ğŸ“Š Vector Search**: ChromaDB-powered semantic search with similarity scoring
- **ğŸ”§ OCR Support**: Tesseract integration for scanned documents and images
- **ğŸŒ RESTful API**: Complete FastAPI application with automatic Swagger documentation
- **ğŸ³ Docker Ready**: Production-ready containerization with Docker Compose
- **ğŸ§ª Comprehensive Testing**: Full test suite with mock providers for development
- **ğŸ“š Complete Documentation**: Extensive examples and API documentation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Document      â”‚    â”‚   Data          â”‚    â”‚   Embeddings    â”‚
â”‚   Ingestion     â”‚â”€â”€â”€â–¶â”‚   Processing   â”‚â”€â”€â”€â–¶â”‚   Generation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   File          â”‚    â”‚   Text          â”‚    â”‚   Vector        â”‚
â”‚   Handlers      â”‚    â”‚   Chunking      â”‚    â”‚   Storage       â”‚
â”‚   (PDF/DOCX/    â”‚    â”‚   (Overlap &    â”‚    â”‚   (ChromaDB)    â”‚
â”‚    Images/OCR)  â”‚    â”‚    Metadata)    â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                       â”‚
                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Query         â”‚â—€â”€â”€â”€â”‚   RAG           â”‚â—€â”€â”€â”€â”‚   Similarity    â”‚
â”‚   Processing    â”‚    â”‚   Pipeline      â”‚    â”‚   Search        â”‚
â”‚   (Natural      â”‚    â”‚   (Context +    â”‚    â”‚   (Vector       â”‚
â”‚    Language)    â”‚    â”‚    Generation)  â”‚    â”‚    Matching)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <repository-url>
cd rag-chatbot-docsearch

# Install dependencies
pip install -r requirements.txt

# Copy environment configuration
cp env.example .env
```

### 2. Configuration

Edit the `.env` file to configure your providers:

```bash
# For OpenAI (requires API key)
OPENAI_API_KEY=your_openai_api_key_here
EMBEDDER_TYPE=openai
LLM_PROVIDER=openai

# For Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_api_key_here
LLM_PROVIDER=anthropic

# For Groq (ultra-fast inference)
GROQ_API_KEY=your_groq_api_key_here
LLM_PROVIDER=groq

# Or use mock providers for testing (no API keys needed)
EMBEDDER_TYPE=mock
LLM_PROVIDER=mock
```

### 3. Run the Demo

```bash
# Run the complete demo
python demo.py

# Or start the API server
python api/main.py
```

### 4. Docker Deployment

```bash
# Start with Docker Compose
docker-compose up -d

# Or build and run manually
docker build -t rag-chatbot .
docker run -p 8000:8000 rag-chatbot
```

## ğŸ“– Usage

### Python SDK

```python
from rag_pipeline import RAGPipeline

# Initialize RAG system
rag = RAGPipeline(
    storage_dir="./storage",
    embedder_type="openai",  # or "mock", "sentence_bert"
    llm_provider="groq"      # or "mock", "openai", "anthropic"
)

# Ingest documents
result = rag.ingest_documents("./documents/")
print(f"Processed {result['total_documents']} documents")

# Query the system
response = rag.query("What is machine learning?")
print(f"Answer: {response['answer']}")
print(f"Sources: {len(response['sources'])}")
print(f"Confidence: {response['confidence']:.2f}")
```

### REST API

#### Upload Documents
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"
```

#### Query Documents
```bash
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is artificial intelligence?", 
    "max_results": 5
  }'
```

#### Ingest from Directory
```bash
curl -X POST "http://localhost:8000/ingest" \
  -H "Content-Type: application/json" \
  -d '{
    "input_path": "/path/to/documents",
    "chunk_size": 1000
  }'
```

#### Health Check
```bash
curl http://localhost:8000/health
```

### API Endpoints

| Endpoint | Method | Description | Parameters |
|----------|--------|-------------|------------|
| `/` | GET | Health check | - |
| `/health` | GET | Service status | - |
| `/upload` | POST | Upload document | `file` (multipart) |
| `/query` | POST | Query documents | `query`, `max_results` |
| `/ingest` | POST | Ingest from path | `input_path`, `chunk_size` |
| `/documents` | GET | List documents | - |
| `/documents` | DELETE | Clear collection | - |

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `STORAGE_DIR` | `./storage` | ChromaDB storage directory |
| `VECTOR_BACKEND` | `chroma` | Vector database backend |
| `EMBEDDER_TYPE` | `mock` | Embedding provider (`mock`, `openai`, `sentence_bert`) |
| `LLM_PROVIDER` | `mock` | LLM provider (`mock`, `openai`, `anthropic`, `groq`) |
| `GROQ_API_KEY` | - | Groq API key for ultra-fast inference |
| `CHUNK_SIZE` | `1000` | Document chunk size |
| `CHUNK_OVERLAP` | `200` | Chunk overlap |
| `API_HOST` | `0.0.0.0` | API server host |
| `API_PORT` | `8000` | API server port |

### Supported File Types

- **Text Files**: `.txt`, `.md`, `.csv`, `.log`
- **PDF Documents**: `.pdf` (with OCR fallback for scanned documents)
- **Office Documents**: `.docx`, `.pptx`, `.xlsx`
- **Images**: `.jpg`, `.png`, `.tiff`, `.bmp` (with OCR text extraction)
- **Archives**: `.zip`, `.tar`, `.tar.gz` (with recursive processing)

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/

# Run specific test file
python -m pytest tests/test_rag_pipeline.py

# Run with coverage
python -m pytest --cov=. tests/

# Run the demo
python demo.py

# Test API endpoints
python test_api.py
```

## ğŸ“ Project Structure

```
rag-chatbot-docsearch/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â””â”€â”€ main.py            # API endpoints and server
â”œâ”€â”€ data_ingestion/        # Document processing pipeline
â”‚   â”œâ”€â”€ loader.py          # Main loader orchestrator
â”‚   â”œâ”€â”€ cleaner.py         # Text cleaning utilities
â”‚   â”œâ”€â”€ ocr_reader.py      # OCR processing
â”‚   â””â”€â”€ pipeline.py        # Processing pipeline
â”œâ”€â”€ embeddings/            # Embedding generation
â”‚   â””â”€â”€ embedder.py        # Multiple embedding providers
â”œâ”€â”€ llm/                   # LLM integration
â”‚   â””â”€â”€ llm_provider.py    # Multiple LLM providers
â”œâ”€â”€ storage/               # Database layer
â”‚   â”œâ”€â”€ db_config.py       # Database configuration
â”‚   â”œâ”€â”€ chroma_adapter.py  # ChromaDB adapter
â”‚   â”œâ”€â”€ raw_store.py       # Raw document storage
â”‚   â””â”€â”€ gold_store.py      # Processed document storage
â”œâ”€â”€ vectorstore/           # Vector operations
â”‚   â””â”€â”€ chroma_store.py    # Vector store wrapper
â”œâ”€â”€ utils/                 # Utilities
â”‚   â””â”€â”€ logger.py          # Logging configuration
â”œâ”€â”€ tests/                 # Test suite
â”‚   â”œâ”€â”€ test_rag_pipeline.py
â”‚   â”œâ”€â”€ test_chroma_adapter_smoke.py
â”‚   â””â”€â”€ test_loader_*.py
â”œâ”€â”€ examples/              # Usage examples
â”‚   â”œâ”€â”€ chromadb_usage.py
â”‚   â”œâ”€â”€ complete_example.py
â”‚   â””â”€â”€ groq_example.py
â”œâ”€â”€ notebooks/             # Jupyter notebooks
â”‚   â””â”€â”€ test.ipynb
â”œâ”€â”€ rag_pipeline.py       # Complete RAG pipeline
â”œâ”€â”€ demo.py               # Interactive demo
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ Dockerfile           # Docker configuration
â”œâ”€â”€ docker-compose.yml   # Docker Compose setup
â”œâ”€â”€ env.example          # Environment template
â”œâ”€â”€ CHROMADB_SETUP.md    # ChromaDB setup guide
â”œâ”€â”€ PROJECT_COMPLETION.md # Project completion status
â””â”€â”€ README.md            # This file
```

## ğŸ”Œ Extending the System

### Adding New Document Handlers

```python
from data_ingestion.loader import Handler

class CustomHandler(Handler):
    def inspect(self, fd):
        return {"page_count": 1, "has_embedded_text": True}
    
    def extract(self, fd):
        # Your extraction logic
        yield {"content": "extracted text", "metadata": {}}
```

### Adding New Embedding Providers

```python
from embeddings.embedder import Embedder

class CustomEmbedder(Embedder):
    def embed_text(self, text):
        # Your embedding logic
        return [0.1] * 384
    
    def embed_batch(self, texts):
        return [self.embed_text(text) for text in texts]
```

### Adding New LLM Providers

```python
from llm.llm_provider import LLMProvider

class CustomLLMProvider(LLMProvider):
    def generate_response(self, prompt, context=None):
        # Your LLM logic
        return "Generated response"
```

## ğŸ³ Docker Deployment

### Using Docker Compose (Recommended)

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

### Using Docker

```bash
# Build image
docker build -t rag-chatbot .

# Run container with volume mounts
docker run -p 8000:8000 \
  -v $(pwd)/storage:/app/storage \
  -v $(pwd)/data:/app/data \
  rag-chatbot
```

## ğŸš¨ Troubleshooting

### Common Issues

1. **ChromaDB Connection Error**
   ```bash
   # Ensure storage directory exists
   mkdir -p storage
   ```

2. **Missing Dependencies**
   ```bash
   # Install all requirements
   pip install -r requirements.txt
   ```

3. **OCR Not Working**
   ```bash
   # Install Tesseract
   # Ubuntu/Debian:
   sudo apt-get install tesseract-ocr
   # macOS:
   brew install tesseract
   # Windows:
   # Download from: https://github.com/UB-Mannheim/tesseract/wiki
   ```

4. **Memory Issues**
   ```bash
   # Reduce chunk size in .env
   CHUNK_SIZE=500
   ```

5. **API Key Issues**
   ```bash
   # Check your .env file has correct API keys
   # Use mock providers for testing without API keys
   EMBEDDER_TYPE=mock
   LLM_PROVIDER=mock
   ```

### Debug Mode

Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ¯ Use Cases

- **Document Q&A**: Ask questions about uploaded documents
- **Knowledge Base**: Build searchable knowledge bases from documents
- **Research Assistant**: Help researchers find relevant information
- **Customer Support**: Answer questions based on documentation
- **Content Analysis**: Analyze and summarize document content

## ğŸ”— Related Documentation

- [ChromaDB Setup Guide](CHROMADB_SETUP.md) - Detailed ChromaDB configuration
- [Project Completion Status](PROJECT_COMPLETION.md) - Implementation details
- [Issues and Fixes](ISSUES_AND_FIXES.md) - Known issues and solutions

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request



## ğŸ‰ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/) for the API
- Powered by [ChromaDB](https://www.trychroma.com/) for vector storage
- Uses [Tesseract OCR](https://github.com/tesseract-ocr/tesseract) for document processing
- Supports multiple LLM providers for flexibility